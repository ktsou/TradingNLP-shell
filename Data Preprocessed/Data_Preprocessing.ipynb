{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61460193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom libraries\n",
    "from data_import import *\n",
    "from data_preprocess import *\n",
    "from data_preprocess_BERT import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e7a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set filepath constants\n",
    "NLP_TOPIC_RECOGNITION_FILEPATH = '../NLP Topic Recognition'\n",
    "#NLP_MODEL_FILEPATH = '../NLP Model Training/model_100K_glove'\n",
    "NLP_MODEL_FILEPATH = '../NLP Model Training/model_100K_glove_91'\n",
    "TEXTUAL_RAW_DATA_FILEPATH = '../Data/textual_raw'\n",
    "\n",
    "## Glove 0.91\n",
    "OUTPUT_DATASET_FILEPATH = \"Dataset_sentiment_100K_91_glove_filtered_full.csv\"\n",
    "# OUTPUT_DATASET_FILEPATH = \"Dataset_sentiment_100K_glove_91_filtered_full_user_averaged.csv\"\n",
    "# OUTPUT_DATASET_FILEPATH = \"Dataset_sentiment_100K_glove_91_filtered_full_user_weights_user_averaged.csv\"\n",
    "\n",
    "## Glove standar\n",
    "# OUTPUT_DATASET_FILEPATH = \"Dataset_sentiment_100K_glove_filtered_full_user_weights_user_averaged.csv\"\n",
    "# OUTPUT_DATASET_FILEPATH = \"Dataset_sentiment_100K_glove_filtered_full_user_weights.csv\"\n",
    "# OUTPUT_DATASET_FILEPATH = \"Dataset_sentiment_100K_glove_filtered_2g_user_weights.csv\"\n",
    "\n",
    "## BERT\n",
    "# OUTPUT_DATASET_FILEPATH_PREV = \"Dataset_sentiment_BERT_trainable.csv\"\n",
    "# OUTPUT_DATASET_FILEPATH = \"Dataset_sentiment_BERT_trainable_full.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06fdf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.append(NLP_TOPIC_RECOGNITION_FILEPATH)\n",
    "from topic_recognition import *\n",
    "#from NLP_class import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(NLP_MODEL_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary of the form {'username': (sentiment_score_sum, occurances)}\n",
    "def get_users(paths):\n",
    "    df = pd.DataFrame()\n",
    "    for path in paths:\n",
    "        text_data = preprocess_textual(path)\n",
    "        df = pd.concat([df, text_data])\n",
    "    \n",
    "    #take the set of all usernames\n",
    "    ini_set = set(df['username'].values)\n",
    "    #initialized as neutral (0.5) and 1 occurance\n",
    "    users = dict.fromkeys(ini_set, (0.5,1))\n",
    "    return users\n",
    "\n",
    "#updates username's sentiment score values after recieving username's tweet\n",
    "def user_update(username, sentiment):\n",
    "    #update \n",
    "    (s,t) = users[username]\n",
    "    (s_new, t_new) = (s + sentiment, t + 1)\n",
    "    users[username] = (s_new, t_new)\n",
    "    \n",
    "    #sentiment calculation sentiment_weighted has range(-1, 1)\n",
    "    #the extreme values are seen when a positive (on average) user tweets something really negative and the converse\n",
    "    sentiment_weighted = sentiment - s/t\n",
    "    return sentiment_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe1587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a bert_preprocess object that automatically sets tokenizers and pretrained bert model classifier\n",
    "bert = BERT_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lol = text.df[:100]\n",
    "# lol['sentiment']=bert.predict(list(lol['text'].values))\n",
    "# pd.options.display.max_colwidth = 400\n",
    "# print(lol[['sentiment','text']])\n",
    "# lol = text.df\n",
    "# list(text.df['text'].values)\n",
    "# start = timer()\n",
    "# lol.text.map(lambda x: bert.predict([x]))\n",
    "# print(timer() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f4b24",
   "metadata": {},
   "source": [
    "# Sentiment analysis on Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89483a8",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch btc price data\n",
    "btc_data = preprocess_BTC()\n",
    "\n",
    "#itterate folder for all raw textual data\n",
    "txt_folder = Path(TEXTUAL_RAW_DATA_FILEPATH).rglob('*.csv')\n",
    "paths = [x for x in txt_folder]\n",
    "\n",
    "#initialize username dictionary for user wieghted sentiment scores\n",
    "#users = get_users(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_folder = Path(TEXTUAL_RAW_DATA_FILEPATH).rglob('*.csv')\n",
    "paths = [x for x in txt_folder]\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89774c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d386e831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "# final dataframe (initialize or load)\n",
    "# df = pd.DataFrame()\n",
    "df = pd.read_csv(OUTPUT_DATASET_FILEPATH, index_col = 0)\n",
    "print(df)\n",
    "start = timer()\n",
    "for path in paths:\n",
    "#     if i<47:\n",
    "#         i+=1\n",
    "#         continue\n",
    "    message = \"Iterration \" + str(i) + \" of \" + str(len(paths))\n",
    "    print(message)\n",
    "    text_data = preprocess_textual(path)\n",
    "    \n",
    "    #initialize an NLP_preprocess object for the current dataset\n",
    "    text = NLP_preprocess(text_data)\n",
    "    \n",
    "    #print(text.df.shape[0])\n",
    "    #perform some spam_filtering on the current dataset\n",
    "    text.spam_filtering()\n",
    "    #print(text.df.shape[0])\n",
    "    \n",
    "    #calculate the sentiment score for every tweet in the current dataset\n",
    "    text.df['sentiment'] = bert.predict(list(text.df['text'].values))\n",
    "    \n",
    "    #add an \"hour\" column that contains the datetime in hourly resolution\n",
    "    text.df[\"hour\"] =  text.df.date.map(lambda x: x[:-6])\n",
    "    #print(text.df)\n",
    "   \n",
    "    #get the mean and median sentiment per hour\n",
    "    timetable = pd.DataFrame(text.df.groupby(['hour']).sentiment.apply(np.mean).values, columns = [\"Bitcoin\"])\n",
    "    timetable2 = pd.DataFrame(text.df.groupby(['hour']).sentiment.apply(np.median).values, columns = [\"Bitcoin_median\"])\n",
    "    \n",
    "    #sync the price and sentiment dataset so that on each row price follows the sentiment\n",
    "    timetable = timetable[:-1]\n",
    "    timetable2 = timetable2[:-1]\n",
    "    btc_connected = connect_datasets(text.df, btc_data)[1:]\n",
    "    \n",
    "    i += 1\n",
    "    #if mismatching sizes disregard current dataset\n",
    "    if len(timetable.index) != len(btc_connected.index):\n",
    "        continue\n",
    "    #otherwise combine them into a single dataset \n",
    "    timetable.index = btc_connected.index\n",
    "    timetable2.index = btc_connected.index\n",
    "    timetable = pd.concat([timetable['Bitcoin'], timetable2['Bitcoin_median'], btc_connected['Open']], axis = 1)\n",
    "    #print(timetable)\n",
    "    \n",
    "    df = pd.concat([df, timetable], axis = 0)   \n",
    "    print(df)\n",
    "    \n",
    "    #save final dataset\n",
    "    df.to_csv(OUTPUT_DATASET_FILEPATH)\n",
    "print(timer() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUTPUT_DATASET_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69750b",
   "metadata": {},
   "source": [
    "## Plain Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4affa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch btc price data\n",
    "btc_data = preprocess_BTC()\n",
    "\n",
    "#itterate folder for all raw textual data\n",
    "# txt_folder = Path(TEXTUAL_RAW_DATA_FILEPATH).rglob('*.csv')\n",
    "# paths = [x for x in txt_folder]\n",
    "\n",
    "#initialize username dictionary for user wieghted sentiment scores\n",
    "#users = get_users(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3659e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterration 1 of 65\n",
      "Duplicate messages in a day\n",
      "\"Bitcoin Tumbles Below $4000 - Down 21% From Record High \n",
      "\"North Korean hackers target bitcoin, Facebook bans monetization of offensive content and more on #CrunchReport wit… https://twitter.com/i/web/status/908116090659651584 …\"\n",
      "\"JPMorgan CEO Says That Bitcoin Is a “Fraud” http://ift.tt/2w8a1E7\"\n",
      "\"http://bitzfree.com - Free Bitcoin Cloud Mining http://bitzfree.com\"\n",
      "\"North Korea Is Dodging Sanctions With a Secret Bitcoin Stash - Bloomberg https://www.bloomberg.com/news/articles/2017-09-11/north-korea-hackers-step-up-bitcoin-attacks-amid-rising-tensions …\"\n",
      "\"Bitcoin is falling and I'm pretty sure most people have started dumping :(\"\n",
      "\"Bitcoin is the future\"\n",
      "\"FreeBitco.in - Free Bitcoin Wallet, Faucet, Lottery and Dice! http://freebitco.in/?r=6348901\"\n",
      "\"Argentina Jumps on Bitcoin ATM Bandwagon with 200 Expected in October https://cointelegraph.com/news/argentina-jumps-on-bitcoin-atm-bandwagon-with-200-expected-in-october … via @Cointelegraph\"\n",
      "\"RT TFTCS \"\"RT trendytradepd \"\"JPMorgan's Dimon says bitcoin 'is a fraud' - https://invst.ly/54doh \"\"\"\"\"\n",
      "Over  100  messages from sender in a day\n",
      "\"Re: New faucet: Nice and clean!Good luck with your first site! #bitcoin #btc http://dld.bz/geJfz\"\n",
      "\"Re: RX 560 4GB Ethereum: Quote from: Branko on Today at 06:55:11 AMQuote from: aspirinex on Sept.. #bitcoin #btc http://dld.bz/geKen\"\n",
      "\"RT StakepoolCom \"\"Cuando hay que hacer caso al banco, se le hace y punto (sobre JPMorgan y bitcoin) Check it out! http://ift.tt/2joIGaT #…\"\n",
      "\"RT StakepoolCom \"\"RT JacobAWohl: How North Korea is Using #Bitcoin and other #Cryptocurrency to Defeat UN Sanctions http://offendedamerica.com/north-korea-using-bitcoin-defeat-un-sanctions/ …\"\"\"\n",
      "\"12z4G81R6...Vd2XPpxsW just won 0.0000026 BTC in our Free #lottery. Try our free Lottery: https://yabtcl.com/freeLottery.aspx … #YABTCL #Bitcoin\"\n",
      "\"19kbxxqry...PUtr4qm1e just won 0.0000024 BTC in our Free #lottery. 3 tries daily to win BTC: https://yabtcl.com/freeLottery.aspx … #YABTCL #Bitcoin\"\n",
      "\"http://youtu.be/e_9QouZWUvc \"\"Yes JERRY \"\"Bitcoin's a Fraud\"\".. Did you come up with that all on your own or did you have some help?\"\" #Bitcoin #E…\"\n",
      "\"\"\"2 Weeks \"\" Flair #litecoin #bitcoin #blockchainhttp://youtu.be/u3Tx9Pw1P0s\"\n",
      "\"Opensource Cryto ATM (Supports Litecoin) #litecoin #bitcoin #blockchainhttp://youtu.be/u3Tx9Pw1P0s\"\n",
      "\"Alaska leading the way at home! #litecoin #bitcoin #blockchain http://youtu.be/u3Tx9Pw1P0s\"\n",
      "Texts containing suspicious phrases\n",
      "get free bitcoin pictwittercompsjunuuasj\n",
      "easy free bitcoin freebitcoin winbitcoin pictwittercomchoqiklymm\n",
      "received 000051109 bitcoin instagc free bitcoin\n",
      "free bitcoin\n",
      "sign free bitcoin using magic link bitcoin btc litecoin eth coinbasepictwittercom5bbut1herw\n",
      "get free bitcoin pictwittercom1x9ilqnueo\n",
      "want free bitcoin ethereum litecoin ripple doge several others visit link get …pictwittercom4mj1osp6f4\n",
      "bonus bitcoin free bitcoin faucet claim 5000 satoshi every 15 minutes … bitcoin faucet via bonusbit\n",
      "miss daily free bitcoin must\n",
      "signup get free bitcoin\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a3f0b3ff1a0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m#apply the model on the padded sequences to get sentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sentiment\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_padded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;31m#print(text.df.index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#itterate folder for all raw textual data\n",
    "txt_folder = Path(TEXTUAL_RAW_DATA_FILEPATH).rglob('*.csv')\n",
    "paths = [x for x in txt_folder]\n",
    "\n",
    "i = 1\n",
    "\n",
    "# final dataframe\n",
    "df = pd.DataFrame()\n",
    "weighted = False\n",
    "\n",
    "start = timer()\n",
    "for path in paths:\n",
    "\n",
    "    message = \"Iterration \" + str(i) + \" of \" + str(len(paths))\n",
    "    print(message)\n",
    "    text_data = preprocess_textual(path)\n",
    "    \n",
    "#     old approach\n",
    "#     timetable = topic_recognition(text_data, flagged_usernames = [], display = message)\n",
    "#     print(timetable)\n",
    "    \n",
    "    #initialize an NLP_preprocess object for the current dataset\n",
    "    text = NLP_preprocess(text_data)\n",
    "    \n",
    "    #print(text.df.shape[0])\n",
    "    text.flag_users()\n",
    "    \n",
    "    \n",
    "    #basic nlp preprocessing (rm stopwords, rm punctuation etc.)\n",
    "    text.preprocess_data()\n",
    "    #perform some spam filtering\n",
    "    text.spam_filtering()\n",
    "    \n",
    "    #print(text.df.shape[0])\n",
    "    \n",
    "    #import default tokenizer for the model\n",
    "    text.import_tokenizer()\n",
    "    #tokenize and pad text sequences of the dataset\n",
    "    df_padded = text.tokenize_and_pad(train = False)\n",
    "        \n",
    "    #apply the model on the padded sequences to get sentiment\n",
    "    text.df[\"sentiment\"] = (model.predict(df_padded))\n",
    "    #print(text.df.index)\n",
    "    \n",
    "#     for r in np.random.randint(0,text.df.shape[0],10):\n",
    "#         print(text.df.iloc[r]['text'], text.df.iloc[r]['sentiment'])\n",
    "\n",
    "\n",
    "    #try: weight messeges by the average optimism/pessimism of each user\n",
    "    #this might have an impact because there are about 5.5 messages per user,\n",
    "    #so there is some history about them\n",
    "    if weighted:\n",
    "        text.df['weighted'] = text.df.index.map(lambda x: user_update(text.df.loc[x]['username'],  text.df.loc[x]['sentiment'])) \n",
    "    #print(text.df)\n",
    "    #add an \"hour\" column that contains the datetime in hourly resolution\n",
    "    text.df[\"hour\"] =  text.df.date.map(lambda x: x[:-6])\n",
    "    \n",
    "    #old\n",
    "#     timetable = pd.DataFrame(text.df.groupby(['hour']).sentiment.apply(np.mean).values, columns = [\"Bitcoin\"])\n",
    "#     timetable2 = pd.DataFrame(text.df.groupby(['hour']).sentiment.apply(np.median).values, columns = [\"Bitcoin_median\"])\n",
    "    \n",
    "\n",
    "    #and then get the mean and median weighted sentiment per hour\n",
    "    if weighted:\n",
    "        #we can eliminate the big effect of users that sent multiple messages by first averaging per user per hour\n",
    "        user_averaged = text.df.groupby(['hour', 'username'], as_index=False).weighted.apply(np.mean)\n",
    "        timetable = pd.DataFrame(user_averaged.groupby(['hour']).weighted.apply(np.mean).values, columns = [\"Bitcoin\"])\n",
    "        timetable2 = pd.DataFrame(user_averaged.groupby(['hour']).weighted.apply(np.median).values, columns = [\"Bitcoin_median\"])\n",
    "    else:\n",
    "        #we can eliminate the big effect of users that sent multiple messages by first averaging per user per hour\n",
    "        user_averaged = text.df.groupby(['hour', 'username'], as_index=False).sentiment.apply(np.mean)\n",
    "#         timetable = pd.DataFrame(user_averaged.groupby(['hour']).sentiment.apply(np.mean).values, columns = [\"Bitcoin\"])\n",
    "#         timetable2 = pd.DataFrame(user_averaged.groupby(['hour']).sentiment.apply(np.median).values, columns = [\"Bitcoin_median\"])\n",
    "        \n",
    "        timetable = pd.DataFrame(text.df.groupby(['hour']).sentiment.apply(np.mean).values, columns = [\"Bitcoin\"])\n",
    "        timetable2 = pd.DataFrame(text.df.groupby(['hour']).sentiment.apply(np.median).values, columns = [\"Bitcoin_median\"])\n",
    "    \n",
    "    #get the mean and median weighted sentiment per hour\n",
    "#     timetable = pd.DataFrame(text.df.groupby(['hour']).weighted.apply(np.mean).values, columns = [\"Bitcoin\"])\n",
    "#     timetable2 = pd.DataFrame(text.df.groupby(['hour']).weighted.apply(np.median).values, columns = [\"Bitcoin_median\"])\n",
    "     \n",
    "    #sync the price and sentiment dataset so that on each row price follows the sentiment\n",
    "    timetable = timetable[:-1]\n",
    "    timetable2 = timetable2[:-1]\n",
    "    btc_connected = connect_datasets(text.df, btc_data)[1:]\n",
    "    \n",
    "    i += 1\n",
    "    #if mismatching sizes disregard current dataset\n",
    "    if len(timetable.index) != len(btc_connected.index):\n",
    "        continue\n",
    "    #otherwise combine them into a single dataset \n",
    "    timetable.index = btc_connected.index\n",
    "    timetable2.index = btc_connected.index\n",
    "    timetable = pd.concat([timetable['Bitcoin'], timetable2['Bitcoin_median'], btc_connected['open'],btc_connected['close']], axis = 1)\n",
    "    print(timetable)\n",
    "    \n",
    "    df = pd.concat([df, timetable], axis = 0)   \n",
    "    #print(df)\n",
    "\n",
    "#save final dataset\n",
    "df.to_csv(OUTPUT_DATASET_FILEPATH)\n",
    "print(timer() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba199a",
   "metadata": {},
   "source": [
    "# Evaluation and Testing (not important for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(OUTPUT_DATASET_FILEPATH, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILEPATH = \"./Dataset_sentiment_100K_glove_filtered_2g.csv\"\n",
    "df2 = pd.read_csv(OUTPUT_DATASET_FILEPATH, index_col = 0)\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "plt.plot(range(0,100), -1*df2['Bitcoin'].values[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d19b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 10))\n",
    "x = -df2['Bitcoin'][100:200]\n",
    "x = x/x[0]\n",
    "y = df2['Open'][100:200]\n",
    "y = y/y[0]\n",
    "plt.plot(range(0,100), x)\n",
    "plt.plot(range(0,100), y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde617c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 10))\n",
    "plt.plot(range(0,100), df.pct_change()['Open'].values[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba4d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randint(0,len(text_data.index))\n",
    "print(text_data.loc[i]['text'])\n",
    "print(text_data.loc[i]['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864551cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BTC price data of the next hour standarized\n",
    "Y = pd.DataFrame(df['Open']).astype(float)\n",
    "#Y = dataset_standarization(Y)\n",
    "\n",
    "#Textual data ('Bitcoin' topic popularity) of the last hour standarized\n",
    "X = pd.DataFrame(df['Bitcoin']).astype(float)\n",
    "#X = dataset_standarization(X)\n",
    "Y.index = X.index\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(Y, X, missing = 'drop').fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec05333",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468dc08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
