{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa8e701",
   "metadata": {
    "id": "p2B9-HFJBOyq"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d4cf7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7JnZmri5j2_",
    "outputId": "818fb176-5d2f-4af4-c977-85d12e6b936d"
   },
   "outputs": [],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4222cfa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXO5FPswBLSH",
    "outputId": "9d331249-1cb7-405b-e589-03d0b7c67436"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow_hub\n",
    "#!pip install keras tf-models-official pydot graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78a170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from official.official import nlp\n",
    "\n",
    "from official.official.nlp import bert\n",
    "\n",
    "from official.official.nlp.bert import tokenization as tokenization\n",
    "from official.official import modeling as modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adaab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "#%load_ext nb_black\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "plt.style.use(style=\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8571e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHzoXf8sCxaW",
    "outputId": "85830d65-5942-4fc1-c4ce-4ae1830812bb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from official import nlp\n",
    "\n",
    "# from official.modeling import tf_utils\n",
    "# from official.nlp import bert\n",
    "# import official.nlp.bert.bert_models\n",
    "# import official.nlp.bert.configs\n",
    "# import official.nlp.bert.run_classifier\n",
    "# import official.nlp.bert.tokenization as tokenization\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912723e",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_LABELLED_DATA_FILEPATH = '..\\Data\\Sentiment Analysis Dataset.csv'\n",
    "txt_data = pd.read_csv(SENTIMENT_LABELLED_DATA_FILEPATH , sep='|', names=['col1'])\n",
    "\n",
    "#split data into columns with ','\n",
    "txt_data = txt_data.col1.str.split(',',  3, expand=True)\n",
    "txt_data.columns = list(txt_data.iloc[0])\n",
    "txt_data = txt_data.drop(0)\n",
    "txt_data.index = np.subtract(txt_data.index, 1)\n",
    "# x = txt_data.groupby('Sentiment')\n",
    "# l=[x.get_group(i)['SentimentText'] for i in x.groups]\n",
    "df = pd.concat([txt_data['SentimentText'], txt_data['Sentiment']], axis = 1)\n",
    "df.columns = ['text', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceac292",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "cYgtLWXODYFJ",
    "outputId": "65aecf27-c816-4d79-b59e-d0c1a8cb0efc"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef50d7",
   "metadata": {
    "id": "5x34_8GvDf9C"
   },
   "source": [
    "We have two classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc03b0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "M8klwmVODcqW",
    "outputId": "74b12285-387a-4556-af33-55e0ce326e9d"
   },
   "outputs": [],
   "source": [
    "df.target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70345062",
   "metadata": {
    "id": "iyohoWzbDj84"
   },
   "source": [
    "Let's check how equally distributed those classes are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191db720",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "bCtMIyUjDh3Z",
    "outputId": "4ce84fd5-97ee-45cd-9940-c11786eb8050"
   },
   "outputs": [],
   "source": [
    "classes = df.target.unique()\n",
    "counts = []\n",
    "\n",
    "for i in classes:\n",
    "  count = len(df[df.target==i])\n",
    "  counts.append(count)\n",
    "\n",
    "plt.bar(['negative', 'positive'], counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a087377d",
   "metadata": {
    "id": "HCwKlyvtD2wB"
   },
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e948a",
   "metadata": {
    "id": "I2bcU8X_Dz9M"
   },
   "outputs": [],
   "source": [
    "sample_size = int(len(df)*0.03)\n",
    "sampleDf = df.sample(sample_size, random_state=23)\n",
    "x = sampleDf.text.values\n",
    "y = sampleDf.target.values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e839b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7b2c1",
   "metadata": {
    "id": "R6CgL-y4EI3o"
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb3156",
   "metadata": {
    "id": "0mmW4jGVDmCH"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y_test = encoder.transform(y_test)\n",
    "encoded_Y_train = encoder.transform(y_train)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_test = np_utils.to_categorical(encoded_Y_test)\n",
    "dummy_y_train = np_utils.to_categorical(encoded_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab5acac",
   "metadata": {
    "id": "cwQNtv17EdUz"
   },
   "outputs": [],
   "source": [
    "encoder_fname = 'twitter_classes.npy'\n",
    "my_wd = './'\n",
    "np.save(os.path.join(my_wd, encoder_fname) , encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835fffb",
   "metadata": {
    "id": "HEopM33sE_Et"
   },
   "outputs": [],
   "source": [
    "# encoder = LabelEncoder()\n",
    "# encoder.classes_ = np.load(os.path.join(my_wd, encoder_fname), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e239c",
   "metadata": {
    "id": "Rq5GSy35Gibn"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607ca63",
   "metadata": {
    "id": "atMa7VWVFQwV"
   },
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2\",\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7f516d",
   "metadata": {
    "id": "VoD6yod_FRyN"
   },
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba2bf9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "d0JoikgjFTmk",
    "outputId": "6fb835af-b863-4d98-eed9-eaecf95de217"
   },
   "outputs": [],
   "source": [
    "do_lower_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33a1658",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "q1Y7q4QTKuTD",
    "outputId": "5f414f0d-0b4b-404d-bc4b-0714b15223df"
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe80ee3",
   "metadata": {
    "id": "-DIT-v6bIFrw"
   },
   "outputs": [],
   "source": [
    "def encode_names(n):\n",
    "   tokens = list(tokenizer.tokenize(n))\n",
    "   tokens.append('[SEP]')  # seperation token. Would bemuch more useful if you had a multiple text input.\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "tweets = tf.ragged.constant([\n",
    "    encode_names(n) for n in x_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4780d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jfcVehzDK_--",
    "outputId": "f9b1b310-15ce-4e3b-cda3-18bdb68c8efc"
   },
   "outputs": [],
   "source": [
    "print('Tokenized Tweets shape', tweets.shape.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b65f191",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "id": "aQ9_HEAiLXUI",
    "outputId": "d405bc1b-d9c5-42e3-f809-d43f30f68576"
   },
   "outputs": [],
   "source": [
    "tokenizedTweet = tokenizer.tokenize(x_train[0])\n",
    "for i in tokenizedTweet:\n",
    "  print(i, tokenizer.convert_tokens_to_ids([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e7aa8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "XOenrketMSKq",
    "outputId": "7c802c11-c4eb-4c77-f4b5-e556fe0595a7"
   },
   "outputs": [],
   "source": [
    "cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*tweets.shape[0]\n",
    "input_word_ids = tf.concat([cls, tweets], axis=-1)\n",
    "_ = plt.pcolormesh(input_word_ids[0:10].to_tensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3cd3c",
   "metadata": {
    "id": "7dKZpLlvQznW"
   },
   "source": [
    "## Mask and input type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f6938",
   "metadata": {
    "id": "DElTR00ENbYf"
   },
   "outputs": [],
   "source": [
    "input_word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b3360a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "PNCmIxTbQ3L-",
    "outputId": "ef0939c3-7cc8-41de-c1b8-033644a9d7ce"
   },
   "outputs": [],
   "source": [
    "input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "plt.pcolormesh(input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0224337a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "4-eSiqx-Q6t7",
    "outputId": "21f4daf8-3eb1-48d5-c66b-29e0284c3379"
   },
   "outputs": [],
   "source": [
    "type_cls = tf.zeros_like(cls)\n",
    "type_tweet = tf.ones_like(tweets)\n",
    "input_type_ids = tf.concat([type_cls, type_tweet], axis=-1).to_tensor()\n",
    "\n",
    "plt.pcolormesh(input_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2fb95c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "wPy0lCwtRs1f",
    "outputId": "4bc8d438-56ee-4b52-d1cd-39f796925238"
   },
   "outputs": [],
   "source": [
    "input_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19899bd5",
   "metadata": {
    "id": "EkRhOGvoSrTi"
   },
   "source": [
    "## Remake into a function for normal use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc94eb59",
   "metadata": {
    "id": "e6gshuXmRwJ8"
   },
   "outputs": [],
   "source": [
    "lens = [len(i) for i in input_word_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719f2ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-Yb9EjKDTNhj",
    "outputId": "b1dc3ffa-5020-4283-b436-dcbbaa9dc1ff"
   },
   "outputs": [],
   "source": [
    "max_seq_length = max(lens)\n",
    "print('Max length is:', max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb46001",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "1-MsePX3Zq0E",
    "outputId": "41bc825d-7621-4e38-c87b-1e212e06d36f"
   },
   "outputs": [],
   "source": [
    "max_seq_length = int(1.5*max_seq_length)\n",
    "print('Max length is:', max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e46f9",
   "metadata": {
    "id": "YYTddxJZT4cS"
   },
   "outputs": [],
   "source": [
    "def encode_names(n, tokenizer):\n",
    "   tokens = list(tokenizer.tokenize(n))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(string_list, tokenizer, max_seq_length):\n",
    "  num_examples = len(string_list)\n",
    "  \n",
    "  string_tokens = tf.ragged.constant([\n",
    "      encode_names(n, tokenizer) for n in np.array(string_list)])\n",
    "\n",
    "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*string_tokens.shape[0]\n",
    "  input_word_ids = tf.concat([cls, string_tokens], axis=-1)\n",
    "\n",
    "  input_mask = tf.ones_like(input_word_ids).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "  type_cls = tf.zeros_like(cls)\n",
    "  type_tokens = tf.ones_like(string_tokens)\n",
    "  input_type_ids = tf.concat(\n",
    "      [type_cls, type_tokens], axis=-1).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "  inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(shape=(None, max_seq_length)),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids}\n",
    "\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed6c8a",
   "metadata": {
    "id": "8K_R_A3lULSO"
   },
   "source": [
    "And now we preprocess inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49808998",
   "metadata": {
    "id": "3X6EQXxVUi4s"
   },
   "outputs": [],
   "source": [
    "X_train = bert_encode(x_train, tokenizer, max_seq_length)\n",
    "X_test = bert_encode(x_test, tokenizer, max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d83c8",
   "metadata": {
    "id": "nJ6PVsxmVbRO"
   },
   "source": [
    "#  MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0608f",
   "metadata": {
    "id": "4QVhnrzH9ygq"
   },
   "source": [
    "## Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a6476",
   "metadata": {
    "id": "TENzkxMeThTv"
   },
   "outputs": [],
   "source": [
    "num_class = len(encoder.classes_)  # Based on available class selection\n",
    "max_seq_length = max_seq_length  # we calculated this a couple cells ago\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])                                  \n",
    "\n",
    "# output = tf.keras.layers.Dense(100, activation='relu')(pooled_output)\n",
    "\n",
    "# output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name='output')(output)\n",
    "\n",
    "output = tf.keras.layers.Dropout(rate=0.1)(pooled_output)\n",
    "output = tf.keras.layers.Dense(768, activation='relu')(output)\n",
    "output = tf.keras.layers.Dense(400, activation='relu')(output)\n",
    "output = tf.keras.layers.Dense(200, activation='relu')(output)\n",
    "output = tf.keras.layers.Dense(100, activation='relu')(output)\n",
    "output = tf.keras.layers.Dense(num_class, activation='sigmoid', name='output')(output)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs={\n",
    "        'input_word_ids': input_word_ids,\n",
    "        'input_mask': input_mask,\n",
    "        'input_type_ids': segment_ids\n",
    "        },\n",
    "        outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9930212",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "id": "oN0Kh1ruUWPF",
    "outputId": "7f3edf43-f673-42a3-ee16-c9f32823fd5c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555580f2",
   "metadata": {
    "id": "bNeskVwpUco0"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 20  # select based on your GPU resources\n",
    "eval_batch_size = batch_size\n",
    "\n",
    "train_data_size = len(dummy_y_train)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=2e-5,\n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "# optimizer = nlp.optimization.create_optimizer(\n",
    "#     2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f84280",
   "metadata": {
    "id": "dpXu5QfFWAoD"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0d9f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "K0-u2YKQWDAV",
    "outputId": "5989c074-e538-40d7-93eb-90ed8e406d4a"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf41eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow-gpu test\n",
    "# import tensorflow as tf\n",
    "# sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50b768",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "ATMWQDkVWY6n",
    "outputId": "5ca423e7-92c6-45ad-9a46-7fd163633abf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train,\n",
    "                    dummy_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_test, dummy_y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b8222",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fname = 'model_BERT_non-trainable'\n",
    "my_wd = './'\n",
    "\n",
    "model.save(os.path.join(my_wd, model_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda3390",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "6yY0h7CPXWD_",
    "outputId": "5e3db6ec-7c47-4d11-f771-838a065bf79c"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train, dummy_y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, dummy_y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f29374",
   "metadata": {
    "id": "n1jJDOPzXcEH"
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f6030",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "RY7XA4HP9S7H",
    "outputId": "e0fa6cce-c75d-43f3-edcb-dc128f1fc108"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231baa4",
   "metadata": {},
   "source": [
    "## Tokenizer load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a862f6",
   "metadata": {
    "id": "9BOIJwPAazvD"
   },
   "outputs": [],
   "source": [
    "tokenizerSaved = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(my_wd, model_fname, 'assets/vocab.txt'),\n",
    "    do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6309b52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "id": "iE3HggMgbkVO",
    "outputId": "6116d77a-0529-4ea0-a247-c904e4a994dd"
   },
   "outputs": [],
   "source": [
    "tokenizedTweet = tokenizerSaved.tokenize(x_train[0])\n",
    "for i in tokenizedTweet:\n",
    "  print(i, tokenizerSaved.convert_tokens_to_ids([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5a289",
   "metadata": {
    "id": "WMhBzgkd5zAM"
   },
   "source": [
    "## Second training itteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6481bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "XaNCm8S6Xd2G",
    "outputId": "d00ad1fb-2514-4dca-fc76-99c0acf05cf0"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bae130",
   "metadata": {
    "id": "UotwRyBE6NP0"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb57c6",
   "metadata": {
    "id": "mXXGcaZM54hg"
   },
   "outputs": [],
   "source": [
    "sample_size = int(len(df)*0.05)\n",
    "sampleDf = df.sample(sample_size, random_state=42)  # notice the random state changes\n",
    "x = sampleDf.text.values\n",
    "y = sampleDf.target.values\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size=0.20, random_state=42)  # notice the random state changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b14b7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "wC2tIF7-67F4",
    "outputId": "d4a94d05-920d-4ba2-d4ca-a2d59d346fc2"
   },
   "outputs": [],
   "source": [
    "classes = sampleDf.target.unique()\n",
    "print(classes)\n",
    "counts = []\n",
    "\n",
    "for i in classes:\n",
    "  count = len(sampleDf[sampleDf.target==i])\n",
    "  counts.append(count)\n",
    "\n",
    "plt.bar(['negative', 'positive'], counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13151be9",
   "metadata": {
    "id": "vAKNqpPh7c2M"
   },
   "source": [
    "#### Label Encoding\n",
    "Now we need to encode labels again. Good thing we have our label encoder saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee49074",
   "metadata": {
    "id": "teAtzXqt7ki-"
   },
   "outputs": [],
   "source": [
    "encoder_fname = 'twitter_classes.npy'\n",
    "my_wd = '/content/drive/My Drive/YouTube/botters_2020-10/twitter_sentiment_course/'\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load(os.path.join(my_wd, encoder_fname), allow_pickle=True)\n",
    "\n",
    "encoded_Y_test2 = encoder.transform(y_test2)\n",
    "encoded_Y_train2 = encoder.transform(y_train2)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_test2 = np_utils.to_categorical(encoded_Y_test2)\n",
    "dummy_y_train2 = np_utils.to_categorical(encoded_Y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b0bc1",
   "metadata": {
    "id": "5c_9kIrI8cDz"
   },
   "source": [
    "#### Input preprocessing\n",
    "As we did before we need to tokenize our inputs (tweets) as `input_word_ids` and then add `input_mask` and `input_type`. As we saved our model, we can use it to build our tokenizer as it was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef22b87",
   "metadata": {
    "id": "lRDJu1jJ8mlF"
   },
   "outputs": [],
   "source": [
    "model_fname = 'twitter_BERT'\n",
    "my_wd = '/content/drive/My Drive/YouTube/botters_2020-10/twitter_sentiment_course/'\n",
    "\n",
    "tokenizerSaved = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(my_wd, model_fname, 'assets/vocab.txt'),\n",
    "    do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89838389",
   "metadata": {
    "id": "wB9ImmkYbxJo"
   },
   "outputs": [],
   "source": [
    "def encode_names(n, tokenizer):\n",
    "   tokens = list(tokenizer.tokenize(n))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(string_list, tokenizer, max_seq_length):\n",
    "  num_examples = len(string_list)\n",
    "  \n",
    "  string_tokens = tf.ragged.constant([\n",
    "      encode_names(n, tokenizer) for n in np.array(string_list)])\n",
    "\n",
    "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*string_tokens.shape[0]\n",
    "  input_word_ids = tf.concat([cls, string_tokens], axis=-1)\n",
    "\n",
    "  input_mask = tf.ones_like(input_word_ids).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "  type_cls = tf.zeros_like(cls)\n",
    "  type_tokens = tf.ones_like(string_tokens)\n",
    "  input_type_ids = tf.concat(\n",
    "      [type_cls, type_tokens], axis=-1).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "  inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(shape=(None, max_seq_length)),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids}\n",
    "\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949a7ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ACSBixa4CUg5",
    "outputId": "53ee8b2c-7a0d-415b-c02d-60837bed802b"
   },
   "outputs": [],
   "source": [
    "print('Max sequence length is:', max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d0be1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "YoKZh0u0CcVT",
    "outputId": "dee019bb-0155-4b1c-a357-f8c16b3807c7"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75b331",
   "metadata": {
    "id": "uiE8pkuqB6k_"
   },
   "outputs": [],
   "source": [
    "X_train2 = bert_encode(x_train2, tokenizerSaved, max_seq_length)\n",
    "X_test2 = bert_encode(x_test2, tokenizerSaved, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73418b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1FG6bmxm6hQo",
    "outputId": "34ab3231-b325-4260-ebdd-095ba8242d02"
   },
   "outputs": [],
   "source": [
    "x_train2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc711087",
   "metadata": {
    "id": "dXHn00cwDWac"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b03fad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "oIcOCiswDN9P",
    "outputId": "0c66274d-9b9e-4eee-ea6f-64e6c289b943"
   },
   "outputs": [],
   "source": [
    "model_fname = 'twitter_BERT'\n",
    "my_wd = '/content/drive/My Drive/YouTube/botters_2020-10/twitter_sentiment_course/'\n",
    "\n",
    "new_model = tf.keras.models.load_model(os.path.join(my_wd, model_fname))\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb19a19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "id": "yapkpJpXIFpw",
    "outputId": "bb17ca99-b780-4321-bf20-7db4f6794e3c"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(new_model, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0169169",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "1n4i6qwPDtlM",
    "outputId": "6f5d9906-fe7c-4168-ab40-7889e422b074"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = new_model.evaluate(X_test, dummy_y_test, verbose=False)  # OLD\n",
    "print(\"Old testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "loss, accuracy = new_model.evaluate(X_test2, dummy_y_test2, verbose=False)  # NEW\n",
    "print(\"New testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77574f",
   "metadata": {
    "id": "TfgAOC5KERoI"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 16  # select based on your GPU resources\n",
    "eval_batch_size = batch_size\n",
    "\n",
    "train_data_size = len(dummy_y_train2)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df508b2",
   "metadata": {
    "id": "mIKL2_TXHKNP"
   },
   "outputs": [],
   "source": [
    "optimizer = nlp.optimization.create_optimizer(\n",
    "    2e-6, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46286de7",
   "metadata": {
    "id": "up0osQrxHntP"
   },
   "outputs": [],
   "source": [
    "new_model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32800614",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "H4_BM84kH03L",
    "outputId": "87741068-0115-42c5-f20f-4b3adbbde8de"
   },
   "outputs": [],
   "source": [
    "history2 = new_model.fit(X_train2,  # using new training set\n",
    "                         dummy_y_train2,  # using new training set\n",
    "                         epochs=epochs,\n",
    "                         batch_size=batch_size,\n",
    "                         validation_data=(X_test, dummy_y_test),  # using old test dataset\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd54d1b",
   "metadata": {
    "id": "cHkRaP7Cpf_V"
   },
   "outputs": [],
   "source": [
    "for i in history2.history:\n",
    "  for ele in history2.history[i]:\n",
    "    history.history[i].append(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48deb4f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "Ug8_LyTbqxcS",
    "outputId": "331c31f4-85a9-467a-beaa-03b4c3c962ef"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea3e84",
   "metadata": {
    "id": "F5y81Wvk1Cav"
   },
   "source": [
    "# Is BERT worth it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571ef04",
   "metadata": {
    "id": "aQIJMSxWRNOX"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f855d186",
   "metadata": {
    "id": "CvXzVKLNS3EZ"
   },
   "source": [
    "We need to check in with our label encoder to get our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace93c8a",
   "metadata": {
    "id": "Q48W5efMSdtI"
   },
   "outputs": [],
   "source": [
    "encoder_fname = 'twitter_classes.npy'\n",
    "my_wd = '/content/drive/My Drive/YouTube/botters_2020-10/twitter_sentiment_course/'\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load(os.path.join(my_wd, encoder_fname), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce30017",
   "metadata": {
    "id": "XgjPsHVcTX9V"
   },
   "source": [
    "This is how our classes are encoded for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab60090",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ZT2DvZ2oTCBJ",
    "outputId": "452e542a-04ea-4665-d26c-72fb71c00e05"
   },
   "outputs": [],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ec934",
   "metadata": {
    "id": "-aVOr8MlSxa6"
   },
   "source": [
    "Input preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b2d97",
   "metadata": {
    "id": "SiruBLfcrCUo"
   },
   "outputs": [],
   "source": [
    "tweet = ['SLEEPY JOE BIDEN IS PROPOSING THE BIGGEST TAX HIKE IN OUR COUNTRIES HISTORY! CAN ANYBODY REALLY VOTE FOR THIS?']\n",
    "inputs = bert_encode(string_list=list(tweet), \n",
    "                     tokenizer=tokenizerSaved, \n",
    "                     max_seq_length=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ac221",
   "metadata": {
    "id": "Zkoy3SgCTdgc"
   },
   "source": [
    "Prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49efade5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQWW5CCtTDji",
    "outputId": "1a503d85-35e8-44cd-ed11-6236e96e4692"
   },
   "outputs": [],
   "source": [
    "prediction = new_model.predict(inputs)\n",
    "print(prediction)\n",
    "print('Tweet is', 'positive' if encoder.classes_[np.argmax(prediction)]==4 else 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5455aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq-DshwxTt4n",
    "outputId": "193c8479-8102-4f65-e3de-d9b1fc9322bd"
   },
   "outputs": [],
   "source": [
    "inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
